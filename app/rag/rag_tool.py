"""Tools for querying the RAG index."""

from __future__ import annotations

from typing import Any
import os

from dotenv import load_dotenv
from llama_index.core.postprocessor import SimilarityPostprocessor
from llama_index.core.response_synthesizers import ResponseMode
from llama_index.core import get_response_synthesizer
from llama_index.core.prompts import PromptTemplate
from llama_index.core.vector_stores.types import VectorStoreQueryMode
from llama_index.llms.openai import OpenAI

from . import get_index

# Load environment variables
load_dotenv()

# Default RAG model
RAG_MODEL = os.getenv("RAG_MODEL")
# Toggle hybrid search to enable sparse + dense retrieval. When disabled, the
# index falls back to semantic search only.
# Enable Qdrant hybrid search when set to "true". Any other value disables it.
RAG_ENABLE_HYBRID = os.getenv("RAG_ENABLE_HYBRID", "false").lower() == "true"

# Prompt for document selection
CHOICE_SELECT_PROMPT = PromptTemplate(
    (
        "A list of documents is shown below. Each document has a number next to it along "
        "with a summary of the document. A question is also provided. \n"
        "Respond with the numbers of the documents you should consult to answer the question, "
        "in order of relevance, as well as the relevance score. The relevance score is a number "
        "from 1-10 based on how relevant you think the document is to the question.\n"
        "Do not include any documents that are not relevant to the question or ranks less than 5.\n"
        "Example format: \n"
        "Document 1:\n<summary of document 1>\n\n"
        "Document 2:\n<summary of document 2>\n\n"
        "...\n\n"
        "Document 10:\n<summary of document 10>\n\n"
        "Question: <question>\n"
        "Answer:\n"
        "Doc: 9, Relevance: 7\n"
        "Doc: 3, Relevance: 4\n"
        "Doc: 7, Relevance: 3\n\n"
        "Let's try this now:\n\n"
        "{context_str}\n"
        "Question: {query_str}\n"
        "Answer:\n"
    )
)

# Global LLM instance for reuse
_llm = OpenAI(model=RAG_MODEL, temperature=0.0)


def _build_query_engine(top_k: int, top_n: int):
    """
    Builds and returns a llama-index QueryEngine configured with:
      - a similarity retriever (top_k)
      - an LLMRerank reranker (top_n)
      - postprocessors [SimilarityPostprocessor, LLMRerank]
    """
    index = get_index()
    postprocessors = [SimilarityPostprocessor(similarity_cutoff=0.50),
#                      LLMRerank(llm=_llm, top_n=top_n, choice_select_prompt=CHOICE_SELECT_PROMPT), #FIXME: muy lento!
                      ]

    response_synthesizer = get_response_synthesizer(
        response_mode=ResponseMode.CONTEXT_ONLY
    )

    if RAG_ENABLE_HYBRID:
        # retrieve top_k sparse, top_k dense, and filter down to
        # (top_k + top_k) / 2 total hybrid results
        return index.as_query_engine(
            llm=_llm,
            node_postprocessors=postprocessors,
            response_synthesizer=response_synthesizer,
            vector_store_query_mode=VectorStoreQueryMode.HYBRID,
            similarity_top_k=int(top_k * 0.7),
            sparse_top_k=int(top_k * 0.3),
            hybrid_top_k=top_k,
            use_async=False,
        )  # TODO: no es posible con qdrant en memoria!
    else:
        return index.as_query_engine(
            llm=_llm,
            node_postprocessors=postprocessors,
            response_synthesizer=response_synthesizer,
            vector_store_query_mode=VectorStoreQueryMode.DEFAULT,
            similarity_top_k=top_k,
            use_async=False,
        )


def query_rag(query: str, top_k: int = 10, top_n: int = 3) -> Any:
    """
    Synchronously queries the RAG index and returns the reranked response.

    :param query: The question text to search for.
    :param top_k: Number of documents to initially retrieve.
    :param top_n: Number of documents to rerank.
    :return: The response generated by the engine.
    """
    response = None

    try:
        engine = _build_query_engine(top_k, top_n)
        return engine.query(query)
    except Exception as e:
        print(f"query_rag exception: {e}")
        return None


async def aquery_rag(query: str, top_k: int = 10, top_n: int = 3) -> Any:
    """
    Asynchronously queries the RAG index and returns the reranked response.

    :param query: The question text to search for.
    :param top_k: Number of documents to initially retrieve.
    :param top_n: Number of documents to rerank.
    :return: The response generated by the engine.
    """

    try:
        engine = _build_query_engine(top_k, top_n)
        return await engine.aquery(query)
    except Exception as e:
        print(f"query_rag exception: {e}")
        return None
